# -*- coding: utf-8 -*-
"""hf_socialNetworkAds_logitmodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/144xtcUD63xbcOVnBnWtKsvFXWmFPnQ8e

## Import Libraries
"""

!pip install huggingface_hub
!pip install transformers
!pip install transformers[torch]
!pip install datasets
!pip install skops
from datasets import load_dataset, load_dataset_builder
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, classification_report
from transformers import Trainer, TrainingArguments
from skops import hub_utils
import pickle
from skops.card import Card, metadata_from_config
from pathlib import Path
from tempfile import mkdtemp, mkstemp

"""## Importing the Dataset

"""

dataset_name = "saifhmb/test"
ds_builder = load_dataset_builder(dataset_name)
ds_builder.info

dataset = load_dataset(dataset_name, split = 'train')

dataset[1]

dataset = pd.DataFrame(dataset)
dataset.head()

X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

"""## Splitting the dataset into the Training set and Test set"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

"""## Feature Scaling"""

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

X_test

"""## Training Logistic Regression Model using the training set"""

model = LogisticRegression()
model.fit(X_train, y_train)

"""## Model explainability"""

!pip install shap

dataset.columns
feature_names = dataset.columns[0:2]
feature_names

import shap
explainer = shap.Explainer(model, X_train, feature_names = feature_names)
shap_values = explainer(X_test)
shap.plots.bar(shap_values)

"""## Predicting the Test set results"""

y_pred = model.predict(X_test)

"""## Making the Confusion Matrix and evaluating performance"""

cm = confusion_matrix(y_pred, y_test, labels=model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
disp.plot()
plt.show()
acc = accuracy_score(y_test, y_pred)
ps = precision_score(y_test, y_pred)
rs = recall_score(y_test, y_pred)
print(f'Model accuracy is {round(acc, 2)}')
print(f'Model precision is {round(ps, 2)}')
print(f'Model recall is {round(rs, 2)}')

"""## Sharing Model to HuggingFace"""

_, pkl_name = mkstemp(prefix="skops-", suffix=".pkl")

with open(pkl_name, mode="bw") as f:
    pickle.dump(model, file=f)

local_repo = mkdtemp(prefix="skops-")

hub_utils.init(
    model=pkl_name,
    requirements=[f"scikit-learn={sklearn.__version__}"],
    dst=local_repo,
    task="tabular-classification",
    data=X_test,
)

model_card = Card(model, metadata=metadata_from_config(Path(local_repo)))

#model_card.add_metrics(**{"accuracy": acc, "precision": ps, "recall": rs})

#disp.figure_.savefig(Path(local_repo) / "confusion_matrix.png")
#model_card.add_plot(
#    **{"Model description/Evaluation Results/Confusion Matrix": "confusion_matrix.png"}
#)

#model_card.save(Path(local_repo) / "README.md")

#repo_id = "saifhmb/social-network-ads-logit-model"
#hub_utils.push(
#  repo_id=repo_id,
#    source=local_repo,
#    token="",
#   commit_message="pushing files to the repo from the example!",
#    create_remote=True,)
